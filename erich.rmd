---
output:
  html_document: default
  pdf_document: default
---
# Métodos de Treino Baseados em Árvores (Erich Müller Dutra - 4908)

Neste capítulo será estudado de forma mais aprofundada modelos de
treinamento baseados em árvores, os quais são simples para
interpretação, como: árvores de decisão e regressão, florestas
aleatórias, adaboost, entre outros. O objetivo é entender o
funcionamento dos mesmos, assim como os critérios que utilizam para
classificarem as amostras.

É importante deixar claro que para utilizarmos esses métodos podemos
usar tanto dados numéricos quanto categóricos. Além disso, não é
necessário padronizar os dados.

## Árvores de Decisão

Uma árvore de decisão, em geral, pergunta uma questão e classifica o
elemento baseado na resposta. Ela utiliza os dados de cada indivíduo
para criar uma regra de separação, que posteriormente será utilizada
para rotular novas amostras.

As árvores de decisão podem ser aplicadas aos problemas de regressão e
classificação. Primeiro vamos considerar os problemas de classificação,
e depois passamos para a regressão.

### em Classificação

Vejamos a seguir um exemplo de árvore de decisão para um problema de
classificação.

**Nomenclatura:**

-   **Nó Raiz** ou **Raiz**: é a variável que se encontra no topo da
    árvore;
-   **Nós Internos** ou **Nós**: são as variáveis intermediárias, que
    possuem tanto setas apontandas para elas como saindo delas;
-   **Nós Folhas** ou **Nós Terminais** ou **Folhas**: possuem apenas
    setas apontadas para elas. Representam a decisão final da árvore.

No processo de construção de uma árvore de decisão é importante
ressaltar que a separação dos dados deve envolver apenas duas respostas:
"Sim" ou "Não". Também é preciso definir a ordem das variáveis, como a
variável com que se deve começar, qual deve ser a seguinte, e assim por
diante. A solução para isso é obtida através do nível de **impureza**
das variáveis.

Dizemos que uma variável é **impura** quando ela não consegue separar
bem os dados em uma árvore de decisão. Para calcularmos a impureza de
uma variável utilizamos o **indíce Gini**, que varia entre 0 (mais puro
possível) e 0,5 (mais impuro possível). Primeiramente calculamos o
índice Gini para cada nó da variável, e em seguida obtemos o índice Gini
da variável como uma média ponderada. O **índice Gini** de um nó é
obtido por: \\\[\\hbox{Gini(nó)} = 1 -- {p_S}\^{2} -- {p_N}\^{2}.\\\]
onde \\(p_S\\) é a proporção de "sim" da resposta da variável de
interesse e \\(p_N\\) a proporção de "não" da resposta da variável de
interesse.

O **índice Gini** da variável é dado pela média do índice Gini para os
nós referentes às respostas "Sim" e "Não" ponderada pela proporção dos
elementos em cada nó.

\\\[ \\hbox{Gini(variável)} = \\hbox{Gini(nó}\_1) \\times P_1 +
\\hbox{Gini(nó}\_2) \\times P_2 \\\]

onde \\(P_1\\) é a proporção de elementos no 1º nó e \\(P_2\\) é a
proporção de elementos no 2º nó.

Vamos construir uma árvore de decisão utilizando a base SmallHeart.

``` wp-block-code
base = readRDS("SmallHeart.rds")
head(base)
```

Nosso objetivo é prever se um indivíduo tem ou não uma doença cardíaca
(variável "HeartDisease"), baseado nas outras variáveis. As variáveis
explicativas são as seguintes:

-   Sex: indica o sexo do indivíduo, onde "M" = Masculino e "F" =
    Feminino;
-   ChestPain: referente ao indivíduo sentir dor no peito, onde
    "typical" = típico e "nontypical" = não típico;
-   Thal: indica se o indivíduo possui Talassemia, onde "normal" = não
    possui, "fixed" = talassemia irreversível e "reversable" =
    talassemia reversível.

Vamos verificar o quão bem as variáveis isoladamente são capazes de
prever se o paciente possui ou não doença cardíaca. Vamos começar pela
variável "Sex".

``` wp-block-code
summary(base$Sex)
```

Note que temos 22 indivíduos do sexo feminino e 50 indivíduos do sexo
masculino. Como a resposta de um nó da árvore deve ser "Sim" ou "Não",
vamos utilizar a variável "Sex=M".

``` wp-block-code
# Verificando quantos indivíduos possuem doença cardíaca de acordo com o sexo:

base %>% group_by(Sex, HeartDisease) %>% summarise(N=n())
```

Então a variável "Sex=M" separa os pacientes da seguinte forma:

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-7.png){.wp-image-185
loading="lazy" width="400" height="340"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-7.png 400w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-7-300x255.png 300w"
sizes="(max-width: 400px) 100vw, 400px"}

Note que a maioria dos pacientes com doença cardíaca terminaram na folha
referente ao sexo masculino, mas a maioria dos que não possuem doença
também. Já podemos ter uma ideia que essa variável não é tão boa em
separar os dados, mas para averiguarmos essa hipótese vamos calcular o
índice gini dela.

Primeiramente vamos calcular o índice Gini do nó "Sex = M Sim":

\\\[ \\hbox{Gini(Sex = M Sim)} = 1- \\left( \\frac{14}{50} \\right)\^{2}
-- \\left( \\frac{36}{50} \\right)\^{2} = 0,403.\\\]

Agora vamos calcular o índice Gini do nó "Sex = M Não":

\\\[ \\hbox{Gini(Sex = M Não)} = 1- \\left( \\frac{2}{22} \\right)\^{2}
-- \\left( \\frac{20}{22} \\right)\^{2} = 0,166.\\\]

O índice Gini da variável "Sex = M" é dado pela média do índice Gini dos
nós referentes às respostas "Sim" e "Não" ponderada pela frequência dos
indivíduos em cada nó.

\\\[ \\hbox{Gini(Sex = M)} = 0,403 \\times \\frac{50}{72} + 0,166
\\times \\frac{22}{72} = 0,331.\\\]

Como o índice Gini da variável "Sex = M" ficou mais próximo de 0,5 do
que de 0, podemos constatar que ela é uma variável com baixa pureza.
Note que se tivéssemos escolhido a variável "Sex = F" o índice Gini
obtido seria o mesmo, pois "Sex = F Sim" é equivalente a "Sex = M Não" e
"Sex = F Não" é equivalente a "Sex = M Sim". ou seja, as contas seriam
as mesmas.

Agora vamos realizar o mesmo processo para a variável "ChestPain", ou
seja, vamos verificar o quão bem ela é capaz de prever se o paciente
possui doença cardíaca.

``` wp-block-code
base %>% group_by(ChestPain) %>% summarise(N=n())
```

Note que temos 23 indivíduos que sentem dor no peito tipicamente e 49
indivíduos que não sentem tipicamente. Vamos verificar quantos deles
possuem doença cardíaca:

``` wp-block-code
base %>% group_by(ChestPain, HeartDisease) %>% summarise(N=n())
```

Vamos considerar a variável "ChestPain = Typical". Ela separa os dados
da seguinte forma:

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image.png){.wp-image-175
loading="lazy" width="454" height="380"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image.png 454w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-300x251.png 300w"
sizes="(max-width: 454px) 100vw, 454px"}

Note que quase metade dos pacientes que possuem dor no peito têm doença
cardíaca. Dos que não sentem a dor no peito, quase \\(\\frac{1}{4}\\)
apenas possui a doença.

Vamos calcular o índice Gini do nó "ChestPain = Typical Sim":

\\\[ \\hbox{Gini(ChestPain = Typical Sim)} = 1- \\left( \\frac{7}{23}
\\right)\^{2} -- \\left( \\frac{16}{23} \\right)\^{2} = 0,423.\\\]

Agora vamos calcular o índice Gini do nó "ChestPain = Typical Não":

\\\[ \\hbox{Gini(ChestPain = Typical Não)} = 1- \\left( \\frac{9}{49}
\\right)\^{2} -- \\left( \\frac{40}{49} \\right)\^{2} = 0,299.\\\]

O índice Gini da variável "ChestPain = Typical" é dado pela média do
índice Gini dos nós referentes às respostas "Sim" e "Não" ponderada pela
frequência dos indivíduos em cada nó.

\\\[ \\hbox{Gini(ChestPain = Typical)} = 0,423 \\times \\frac{23}{72} +
0,299 \\times \\frac{49}{72} = 0,339.\\\]

Note que ela obteve um índice Gini um pouco maior do que a variável "Sex
= M". Isso indica que a variável "Sex = M" é mais pura do que a variável
"ChestPain = Typical".

Agora falta apenas obter o índice Gini da variável "Thal". Mas
diferentemente das outras 2 ela não possui apenas 2 níveis, e sim 3:
"normal", "fixed" e "reversable".

``` wp-block-code
library(dplyr)
base %>% group_by(Thal) %>% summarise(N=n())
```

Nesse caso vamos ter que calcular o índice Gini para todas as
combinações possíveis: "Thal = normal", "Thal = fixed", "Thal =
reversable", "Thal = normal ou fixed", "Thal = normal ou reversable",
"Thal = fixed ou reversable". Porém note que o índice Gini da variável
"Thal = normal" é equivalente ao da variável "Thal = fixed ou
reversable", pois "Thal = normal Sim" é o mesmo que "Thal = fixed ou
reversable Não". Da mesma forma isso vale para as variáveis "Thal =
fixed" e "Thal = normal ou reversable", e "Thal = reversable" e "Thal =
normal ou fixed". Com isso conseguimos economizar algumas contas.

``` wp-block-code
base %>% group_by(Thal, HeartDisease) %>% summarise(N=n())
```

Vamos, primeiramente, olhar para a variável "Thal = normal". Ela separa
os dados da seguinte forma:

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-2.png){.wp-image-177
loading="lazy" width="454" height="380"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-2.png 454w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-2-300x251.png 300w"
sizes="(max-width: 454px) 100vw, 454px"}

Note que a maioria dos pacientes que possuem doença cardíaca estão no
grupo dos que possuem "Thal = normal".

Vamos calcular o índice Gini do nó "Thal = Normal Sim":

\\\[ \\hbox{Gini(Thal = Normal Sim)} = 1- \\left( \\frac{8}{52}
\\right)\^{2} -- \\left( \\frac{44}{52} \\right)\^{2} = 0,26.\\\]

Agora vamos calcular o índice Gini do nó "Thal = Normal Não":

\\\[ \\hbox{Gini(Thal = Normal Não)} = 1- \\left( \\frac{8}{20}
\\right)\^{2} -- \\left( \\frac{12}{20} \\right)\^{2} = 0,48.\\\]

Então o índice Gini da variável "Thal = Normal" fica da seguinte forma:

\\\[ \\hbox{Gini(Thal = Normal)} = 0,26 \\times \\frac{52}{72} + 0,48
\\times \\frac{20}{72} = 0,321.\\\]

Agora vamos olhar para a variável "Thal = Fixed". Ela separa os dados da
seguinte forma:

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-4.png){.wp-image-180
loading="lazy" width="454" height="380"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-4.png 454w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-4-300x251.png 300w"
sizes="(max-width: 454px) 100vw, 454px"}

Vamos calcular o índice Gini do nó "Thal = Fixed Sim":

\\\[ \\hbox{Gini(Thal = Fixed Sim)} = 1- \\left( \\frac{1}{4}
\\right)\^{2} -- \\left( \\frac{3}{4} \\right)\^{2} = 0,375.\\\]

Agora vamos calcular o índice Gini do nó "Thal = Fixed Não":

\\\[ \\hbox{Gini(Thal = Fixed Não)} = 1- \\left( \\frac{15}{68}
\\right)\^{2} -- \\left( \\frac{53}{68} \\right)\^{2} = 0,344.\\\]

Então o índice Gini da variável "Thal = Fixed" fica da seguinte forma:

\\\[ \\hbox{Gini(Thal = Fixed)} = 0,375 \\times \\frac{4}{72} + 0,344
\\times \\frac{68}{72} = 0,346.\\\]

Por último, vamos olhar para a variável "Thal = Reversable".

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-5.png){.wp-image-181
loading="lazy" width="454" height="380"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-5.png 454w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-5-300x251.png 300w"
sizes="(max-width: 454px) 100vw, 454px"}

Vamos calcular o índice Gini do nó "Thal = Reversable Sim":

\\\[ \\hbox{Gini(Thal = Reversable Sim)} = 1- \\left( \\frac{7}{16}
\\right)\^{2} -- \\left( \\frac{9}{16} \\right)\^{2} = 0,492.\\\]

Agora vamos calcular o índice Gini do nó "Thal = Reversable Não":

\\\[ \\hbox{Gini(Thal = Reversable Não)} = 1- \\left( \\frac{9}{56}
\\right)\^{2} -- \\left( \\frac{47}{56} \\right)\^{2} = 0,269.\\\]

Então o índice Gini da variável "Thal = Reversable" fica da seguinte
forma:

\\\[ \\hbox{Gini(Thal = Reversable)} = 0,492 \\times \\frac{16}{72} +
0,269 \\times \\frac{56}{72} = 0,319\\\]

Resumindo, os índices Ginis de todas as variáveis são:

``` wp-block-code
gini = tibble("Variáveis" = c("Sex = M", "ChestPain = Typical", "Thal = Normal", "Thal = Fixed", "Thal = Reversable"), "Índice Gini" = c("0,331", "0,339", "0,321", "0,346", "0,319"))

knitr::kable(gini)
```

A variável "Thal = Reversable" é a que possui o menor índice Gini,
portanto ela é a mais pura. Ela ficará no topo da árvore de decisão, ou
seja, será o nó raiz.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-1.png){.wp-image-176
loading="lazy" width="520" height="420"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-1.png 520w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-1-480x388.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 520px, 100vw"}

O próximo passo é definir as variáveis que ficarão no nó "Thal =
Reversable Sim" e "Thal = Reversable Não". Para isso temos que olhar
para a base de dados com os indivíduos do grupo "Thal = Reversable Sim"
e "Thal = Reversable Não", respectivamente.

``` wp-block-code
# Grupo de indivíduos "Thal = Reversable Sim":
base1 = base %>% filter(Thal == "reversable")
head(base1)
```

Agora temos que calcular o índice Gini para todas as variáveis
referentes a esse grupo. A que for mais pura entrará no nó "Thal =
Reversable Sim". Poupando os cálculos, vamos obter que o menor índice
Gini é o da variável "ChestPain = Typical".

``` wp-block-code
# Grupo de indivíduos "Thal = Reversable Não":
base2 = base %>% filter(Thal != "reversable")
head(base2)
```

Agora calculamos também o índice Gini para todas as variáveis referentes
a esse grupo. Após os cálculos necessários veremos que o menor índice
Gini é o da variável "ChestPain = Nontypical".

Dessa forma, podemos dar continuidade a nossa árvore.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-3-1024x522.png){.wp-image-178
loading="lazy" width="1024" height="522"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-3-980x499.png 980w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-3-480x244.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) and (max-width: 980px) 980px, (min-width: 981px) 1024px, 100vw"}

Após obtidos esses novos nós, o processo continua se repetindo, obtendo
novos nós e/ou folhas para a árvore, até a construção chegar ao fim.

Pergunta: quando o processo de construção de uma árvore chega ao fim? O
processo de construção pode terminar por 3 fatores:

1.  Quando a pureza do nó é maior do que o de qualquer variável que
    adicionamos;
2.  Quando atingimos folhas 100% puras (índice Gini = 0);
3.  Quando o ganho ao aumentar a árvore é muito pequeno.

O ganho ao aumentar a árvore pode ser resumido como um conjunto de
atributos presentes na árvore que retornem o maior ganho de informações.
Essa questão será melhor abordada posteriormente, juntamente com a
questão de como podar as árvores (que está intimamente relacionada ao
ganho) no subcapítulo [XGBoost](#link10).

### em Regressão

Agora iremos discutir o processo de construção de uma árvore de
regressão. Em uma árvore de regressão, diferentemente de uma árvore para
classificação, cada folha possui um valor numérico (ao invés de
categorias como "Sim" ou "Não", como no exemplo anterior da base
SmallHeart). Vejamos a seguir um exemplo de árvore de decisão para um
problema de regressão.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-6.png){.wp-image-182
loading="lazy" width="520" height="420"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-6.png 520w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-6-480x388.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 520px, 100vw"}

Esse valor numérico presente nas folhas não é nada menos que a média do
valor da variável de interesse a ser prevista para os elementos que
satisfazem a condição do nó. Por exemplo, na árvore de regressão acima a
primeira folha dá como resultado uma eficácia de 5%: essa foi a média
observada da eficácia do medicamento em pacientes com mais de 50 anos de
idade. Para a segunda folha, a com eficácia de 20%: esse valor é a média
da eficácia do medicamento em um indivíduo com menos de 50 anos de idade
e que toma uma dosagem maior do que 29mg foi de 20%. O processo é o
mesmo para as outras folhas.

A grande pergunta é qual valor colocar no nó como condição. Para
exemplificar como funciona o processo, vamos começar com um exemplo
simples:

**Ex.:** Vamos carregar o banco de dados "SmallAdvertising". Este banco
possui informações sobre as vendas de um produto em 10 mercados
diferentes (variável sales), além de orçamentos de publicidade para esse
produto em cada um dos mercados para três mídias diferentes: TV, rádio e
jornal (variáveis TV, radio e newspaper, respectivamente).

``` wp-block-code
vendas = readRDS("SmallAdvertising.rds")
vendas
```

Vamos considerar o caso em que queremos construir uma árvore de
regressão para prever as vendas baseados apenas na variável TV.

``` wp-block-code
plot(vendas$TV, vendas$sales, pch = 19,
     xlab = "Orçamento de Publicidade do Produto para a TV",
     ylab = "Vendas do Produto",
     main = "Vendas do produto x Publicidade para a TV")
```

Primeiramente é preciso definir qual valor irá entrar como condição no
primeiro nó. O algoritmo realiza isso testando todos os possíveis
valores de separação para os dados, e pega o que minimiza a soma dos
quadrados dos resíduos. Inicialmente, como o primeiro separador, ele
considera a média dos 2 menores valores da Publicidade.

``` wp-block-code
ordenados = sort(vendas$TV)
mean(ordenados[1:2])
```

Então 44,95 é o primeiro valor a ser testado para a separação dos dados.

``` wp-block-code
plot(vendas$TV, vendas$sales, pch = 19,
     xlab = "Orçamento de Publicidade do Produto para a TV",
     ylab = "Vendas do Produto",
     main = "Vendas do produto x Publicidade para a TV"); abline(v = 44.95,
                                                                 col = "red")
```

Assim, o primeiro nó será da seguinte forma:

\<img
src=\"http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-6.png\"
alt=\"Figura 11: Divisão por \'TV

Para a resposta "sim" prevemos que as vendas do produto será de 9,2, o
qual é o resultado da média dos valores das vendas para todos os
produtos cuja publicidade foi menor do que 44,95 (ou seja, é apenas o
valor do primeiro elemento). Para a resposta "Não", então a folha
seguinte contém o resultado da média dos valores das vendas para todos
os produtos cuja publicidade foi maior do que 44,95, o qual é de 15,05.

Note que fazendo isso teremos resíduos (diferença do valor original e do
valor predito pela árvore) muito grandes. O algoritmo eleva esses
resíduos ao quadrado e os soma. Esse valor é a soma dos quadrados dos
resíduos considerando o nó "Publicidade para a TV \< 44,95?".

Em seguida ele irá para o próximo separador: a média do segundo e do
terceiro menores pontos.

``` wp-block-code
mean(ordenados[2:3])
```

Então 66,95 é o segundo valor a ser testado para a separação dos dados.

``` wp-block-code
plot(vendas$TV, vendas$sales, pch = 19,
     xlab = "Orçamento de Publicidade do Produto para a TV",
     ylab = "Vendas do Produto",
     main = "Vendas do produto x Publicidade para a TV"); abline(v = 66.95,
                                                                 col = "red")
```

Então o nó considerado será da forma "Publicidade para a TV \< 66,95?".

\<img
src=\"http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-8.png\"
alt=\"Figura 12: Divisão por \'TV

O valor de 8,9 corresponde ao resultado da média dos valores das vendas
para todos os produtos cuja publicidade foi menor do que 66,95. Então a
árvore prevê esse valor de vendas para o produto que obteve uma
publicidade para a TV \< 66,95. O valor de 15,77 é o resultado da média
dos valores das vendas para todos os produtos cuja publicidade foi maior
do que 66,95. Novamente serão obtidos os resíduos dessa predição e eles
serão somados.

Então o algoritmo irá para o próximo separador e irá calcular a soma dos
quadrados dos resíduos da predição. Isso ocorre sucessivamente até
acabarem todos os separadores possíveis para a árvore. O separador
vencedor (aquele que irá para o nó raiz) é aquele com a menor soma dos
quadrados dos resíduos.

A construção dos próximos nós se dá pela mesma forma que a do nó raiz. O
processo de construção da árvore termina quando:

1.  Atingimos um número mínimo de observações em uma folha (usualmente é
    utilizado 20 observações). Não continuamos a divisão após esse
    número mínimo pois corremos o risco de criar uma árvore
    sobreajustada à amostra dada;
2.  Quando o ganho ao aumentar a árvore é muito pequeno.

Agora vamos para o caso em que tenhamos mais de uma variável preditiva
nos dados. Vamos considerar agora que queremos prever as vendas do
produto baseado em seus orçamentos de publicidade para TV, rádio e
jornal.

Assim como anteriormente, começamos usando o orçamento para a TV para
prever as vendas, e pegamos o separador com a menor soma dos quadrados
dos resíduos. O melhor separador se torna um candidato para a raiz da
árvore. Em seguida, focamos em utilizar o orçamento para o rádio para
prever as vendas. Assim como com o orçamento para a TV, tentamos
diferentes separadores para a predição e calculamos a soma dos quadrados
dos resíduos em cada passo. O melhor separador se torna outro candidato
para a raiz. Por último, utilizamos o orçamento para o jornal para
prever as vendas, e após tentarmos diferentes separadores pegamos aquele
com a menor soma dos quadrados dos resíduos também. Então comparamos a
soma dos quadrados dos resíduos de todos os candidatos para a raiz, e o
escolhido, novamente, é aquele com a menor soma.

Para os próximos nós o processo de construção também é equivalente ao
anterior, exceto que agora nós comparamos a menor soma dos quadrados dos
resíduos de cada preditor. E, novamente, quando uma folha atinge um
número mínimo de observações, a árvore é finalizada.

### Construindo árvores com o `rpart` e `rpart.plot`

Vamos construir árvores com o comando rpart(). Como argumento da função
nós passamos:

1.  A variável de interesse a ser prevista em função das variáveis
    preditoras;
2.  A base de dados onde as variáveis se encontram.

Vamos utilizar a base de dados referentes ao primeiro exemplo dado de
construção de uma árvore, onde queríamos prever se um indivíduo possui
doença cardíaca baseado em características dele.

``` wp-block-code
library(rpart)
heart_arvore = rpart(HeartDisease~., data = base)
```

Agora vamos plotar a árvore com o comando rpart.plot().

``` wp-block-code
library(rpart.plot)
rpart.plot(heart_arvore)
```

Observe que a árvore ficou "vazia". O que ela quer dizer com isso é:
assuma "Não" sempre para o indivíduo possuir doença cardíaca, e acerte
com precisão de 78%. Isso ocorre devido aos valores iniciais do comando
rpart.control(), que ajusta os parâmetros da função rpart(). Os
principais parâmetros do rpart.control são:

-   minsplit: o número mínimo de observações que devem existir em um nó
    para que uma divisão seja tentada. Padrão: minsplit = 20;
-   minbucket: o número mínimo de observações em qualquer folha. Padrão:
    minbucket = minsplit/3;
-   cp (complexity parameter): o mínimo de ganho de ajuste que devemos
    ter em cada divisão. O principal papel desse parâmetro é economizar
    tempo de computação removendo as divisões que não valem a pena.
    Padrão: cp = 0,01;
-   maxdepth: profundidade máxima da árvore (a profundidade da raiz é
    zero). Não pode ser maior que 30.

**Ex. 1:** Vamos ajustar os parâmetros da árvore e construí-la
novamente. Vamos determinar que a profundidade da árvore seja 2, que 0
seja o número mínimo de observações em um nó e que ela seja construída
mesmo que não haja ganhos em mais divisões.

``` wp-block-code
controle = rpart.control(minsplit=0, cp = -1, maxdepth = 2)
heart_arvore = rpart(HeartDisease~., data = base, control = controle)
rpart.plot(heart_arvore)
```

Note que o nó raiz é exatamente aquele que calculamos como o mais puro,
o "Thal = Reversable", que é equivalente a "Thal = Fixed ou Normal". Os
nós adjacentes também foram o que obtivemos anteriormente como os mais
puros.

Cada saída do comando rpart.plot() tem um significado específico:

1.  A primeira saída é a classe estimada pela árvore para as amostras
    que se encontram naquele nó.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-42.png){.wp-image-226
loading="lazy" width="614" height="326"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-42.png 614w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-42-480x255.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 614px, 100vw"}

1.  A segunda saída é a proporção de indivíduos na classe contrária
    àquela estimada na primeira saída.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-2.jpeg){.wp-image-186
loading="lazy" width="649" height="342"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-2.jpeg 649w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-2-480x253.jpeg 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 649px, 100vw"}

1.  A terceira saída é a porcentagem da amostra que se encontra no atual
    nó.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-15.png){.wp-image-197
loading="lazy" width="600" height="322"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-15.png 600w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-15-480x258.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 600px, 100vw"}

**Ex. 2:** Vamos agora constuir a árvore mais completa possível, ou
seja, uma árvore sobreajustada à amostra, sem restrições em sua
profundidade máxima.

``` wp-block-code
controle = rpart.control(minsplit=0, cp = -1)
heart_arvore = rpart(HeartDisease~., data = base, control = controle)
rpart.plot(heart_arvore)
```

**Ex. 3:** Vamos agora considerar 10 como o número mínimo de observações
em um nó e 3 como a profundidade máxima da árvore.

``` wp-block-code
controle = rpart.control(minsplit=10, cp = -1, maxdepth = 4)
heart_arvore = rpart(HeartDisease~., data = base, control = controle)
rpart.plot(heart_arvore)
```

Agora podemos levantar a seguinte questão: como avaliar a precisão do
modelo construído? Nesse exemplo nós utilizamos toda a amostra para
construir a árvore, apenas para explicar o funcionamente do `rpart`,
então não temos uma amostra teste para verificar o quão bom é o modelo.
Então para isso teríamos que primeiramente dividir a amostra em treino e
teste, depois criar o modelo com a amostra treino e em seguida aplicá-lo
na amostra teste, e então, por último, poderíamos utilizar a função
confusionMatrix() para obtermos não só a precisão como outras medidas
avaliativas do modelo, além, é claro, da matriz de confusão. No tópico
abaixo essas etapas serão construídas detalhadamente.

### Árvores com `train` {#link8}

Podemos utilizar árvores de decisão/regressão como um método de
treinamento para os dados através da função train(). Vamos fazer isso
utilizando a base de dados College. Este banco possui informações sobre
777 diferentes universidades e faculdades dos EUA. Ela apresenta algumas
variáveis como: Apps -- número de pedidos recebidos para ingresso,
Room.Board -- custos de acomodação e alimentação, Books -- custos
estimados de livros, PhD -- quantidade de professores com doutorado,
entre outras, e nossa variável de interesse Private, que indica se a
universidade é privada ou pública.

``` wp-block-code
library(readr)
college = read_csv2("College.csv")
head(college)
```

Vamos, primeiramente, separar a amostra em treino e teste.

``` wp-block-code
library(caret)
set.seed(100)
noTreino = createDataPartition(y = college$Private, p = 0.7, list = F)
treino = college[noTreino,]
teste = college[-noTreino,]
```

Vamos treinar o modelo pelo método de árvores de decisão. Fazemos isso
através do argumento "method = `rpart`" da função train().

``` wp-block-code
set.seed(100)
modelo = caret::train(Private~., method = "rpart", data = treino)
modelo
```

Observe que através do train() são testados alguns valores para o cp
(*complexity parameter*) e é eleito aquele com a maior taxa de acurácia.
Nesse caso, o cp utilizado será o de aproximadamente 0,0436. Vamos
aplicar o modelo no conjunto teste.

``` wp-block-code
predicao = predict(modelo, teste)

# Transformando em fator para depois construirmos a matriz de confusão:
teste$Private = as.factor(teste$Private)

# Avaliando o modelo utilizando a matriz de confusão:
confusionMatrix(predicao, teste$Private)
```

Obtivemos uma acurácia de 0,8922, o que é razoável para um modelo que
utiliza árvores.

``` wp-block-code
# Desenhando a árvore:
rpart.plot(modelo$finalModel)
```

A limitação de utilizar as árvores através do train() é que o único
parâmetro da árvore que pode ser alterado é o cp (*complexity
parameter*).

``` wp-block-code
modelLookup("rpart")
```

Para alterarmos o seu valor utilizamos o comando expand.grid().

``` wp-block-code
controle = expand.grid(.cp = 0.0001)
modelo = caret::train(Private~., method = "rpart", data = treino, tuneGrid = controle)
modelo
```

Note que com esse valor de cp a árvore fica mais profunda, pois estamos
diminuindo o mínimo de ganho de ajuste que devemos ter em cada divisão.

``` wp-block-code
rpart.plot(modelo$finalModel)
```

------------------------------------------------------------------------

## Florestas Aleatórias

As árvores de decisão possuem uma estrutura de fácil compreensão, o que
faz com que ela seja bastante utilizada devido a sua boa aparência e
interpretação intuitíva. Mas elas possuem uma limitação, o
[sobreajuste](#link4), sendo assim, elas não são muito eficientes com
novas amostras. O que fazer então?

As **Florestas Aleatórias** (*Random Forest*) se utilizam de várias
árvores de decisão, combinando a simplicidade das árvores com a
flexibilidade de um método sem sobreajuste, aumentando assim a precisão
do preditor.

Vamos construir uma floresta aleatória usando a base de dados
`balloons`.

``` wp-block-code
balloons = readr::read_csv("balloons.csv")
balloons$Inflated = as.factor(balloons$Inflated)
str(balloons)
```

Com base na cor do balão, o tamanho dele, se ele é elástico ou não e se
quem o está enchendo é uma criança ou um adulto, queremos predizer se o
balão vai encher ou não. Portanto, nossa variável de interesse é
`Inflated` e queremos construir um classificador.

A primeira coisa que precisamos fazer é criar uma nova amostra do mesmo
tamanho da original utilizando [bootstrap](#link5).

``` wp-block-code
set.seed(33)
boot1 = caret::createResample(y=balloons$Inflated, times=1, list=F)
NovaAmostra1 = balloons[boot1,]
Out_of_bag = balloons[-boot1,]
```

Todas as observações que não forem sorteadas vão entrar no
"*Out-of-Bag*". Temos 4 variáveis fora a de interesse, vamos sortear 2
variáveis para construir o primeiro nó da nossa árvore.

``` wp-block-code
set.seed(413)
sample(1:4, 2)
```

Vamos calcular o índice Gini para essas duas variáveis.

``` wp-block-code
#calculando o indice gini para a variável tamanho
table(NovaAmostra1$Size, NovaAmostra1$Inflated)
(gini.size = (1-(7/14)^2-(7/14)^2)*(14/20) + (1-(4/6)^2-(2/6)^2)*(6/20))
#calculando o indice gini para a variável idade
table(NovaAmostra1$Age, NovaAmostra1$Inflated)
(gini.age = (1-(5/14)^2-(9/14)^2)*(14/20) + (1-(6/6)^2-(0/6)^2)*(6/20))
```

A variável idade tem um grau de impureza menor, então ela será a raiz da
árvore.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-4-1024x466.jpeg){.wp-image-190
loading="lazy" width="1024" height="466"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-4-980x446.jpeg 980w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-4-480x219.jpeg 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) and (max-width: 980px) 980px, (min-width: 981px) 1024px, 100vw"}

{width=60%}

Agora das variáveis que ainda não foram usadas, sorteamos mais duas para
continuar a árvore.

``` wp-block-code
set.seed(443)
sample(1:3, 2)
```

``` wp-block-code
library(dplyr)
NovaAmostra1 = filter(NovaAmostra1, Age=="ADULT") 
#calculando o indice gini para a variável tamanho
table(NovaAmostra1$Size, NovaAmostra1$Inflated)
(gini.size = (1-(4/11)^2-(7/11)^2)*(11/14) + (1-(1/3)^2-(2/3)^2)*(3/14))
#calculando o indice gini para a variável act
table(NovaAmostra1$Act, NovaAmostra1$Inflated)
(gini.act = (1-(5/5)^2-(0/5)^2)*(5/14) + (1-(0/9)^2-(9/9)^2)*(9/14))
```

Como a variável *act* tem o menor grau de impureza, ela será o próximo
nó.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-7-1024x593.jpeg){.wp-image-215
loading="lazy" width="1024" height="593"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-7-980x568.jpeg 980w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-7-480x278.jpeg 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) and (max-width: 980px) 980px, (min-width: 981px) 1024px, 100vw"}

{width=60%}

Assim, temos nossa primeira árvore de decisão.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-3-1024x615.jpeg){.wp-image-189
loading="lazy" width="1024" height="615"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-3-980x588.jpeg 980w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-3-480x288.jpeg 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) and (max-width: 980px) 980px, (min-width: 981px) 1024px, 100vw"}

{width=60%}

Em seguida vamos construir várias árvores da mesma maneira que a
anterior. Para nosso exemplo vamos construir apenas 4 árvores, mas em
geral vamos fazer bem mais que isso.

Temos então nossas 4 árvores construidas.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-5-1024x372.jpeg){.wp-image-192
loading="lazy" width="1024" height="372"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-5-980x356.jpeg 980w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-5-480x175.jpeg 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) and (max-width: 980px) 980px, (min-width: 981px) 1024px, 100vw"}

{width=90%}

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-6-1024x377.jpeg){.wp-image-211
loading="lazy" width="1024" height="377"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-6-980x360.jpeg 980w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-6-480x177.jpeg 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) and (max-width: 980px) 980px, (min-width: 981px) 1024px, 100vw"}

{width=90%}

Para classificar uma nova amostra, devemos passar ela por todas as
árvores construidas e rotular a amostra pela categoria resultada mais
vezes.

> O método de usar bootstrap para criar novas amostras e votos para a
> tomada de decisão é chamado de ***Bagging***
> (*\*\*Bootstrap+aggregate*).

As observações de cada amostra que não entraram na construção de cada
árvore estão contidas `Out_of_bag`. Essas observações servirão para
avaliar nosso preditor.

``` wp-block-code
Out_of_bag = balloons[c(2,4,12,13,15,18,20,
                        1,2,3,5,10,
                        2,4,12,13,15,18,20,
                        2,3,11,13,14,16,19),]
knitr::kable(Out_of_bag)
```

Para avaliar, é preciso passar cada uma das observações do `Out_of_bag`
por todas as árvores e a predição será feita por votos também. Ao fazer
isso, observamos uma [precisão](#link6) de 86%.

> A proporção de amostras do Out-of-bag que foram incorretamente
> classificadas é chamada *Out-of-bag-error*

Agora que sabemos avaliar o modelo, podemos comparar florestas
aleatórias construídas com 2 variáveis com as construídas com 3 e outras
diferentes configurações. Tipicamente, começamos usando o quadrado do
número de variáveis da base e tentamos algumas quantidades abaixo e
acima.

### Construindo uma floresta com o `randomForest()`

O pacote `randomForest` possui as ferramentas adequadas para a criação
de uma floresta aleatória. Vamos construir uma floresta com 20 árvores
utilizando a base `balloons`.

> É importante observar se as váriaveis categóricas estão na classe de
> fatores.

``` wp-block-code
balloons = readr::read_csv("balloons.csv")
# tratando todas as variaveis 
balloons = dplyr::mutate_if(balloons, is.character, as.factor)
balloons$Inflated = as.factor(balloons$Inflated)
# construindo floresta com 20 arvores
library(randomForest)
set.seed(23)
modelo = randomForest(Inflated ~ ., data=balloons, ntree=20)
```

Agora, vamos avaliar a precisão do modelo.

``` wp-block-code
# avaliando o modelo
modelo
```

> Note que foram construídas 20 árvores utilizando 2 variáveis a cada
> vez. Essa quantidade de variáveis pode ser alterada usando o argumento
> `mtry=` dentro do randomForest.

Podemos ver que a precisão do nosso modelo é de 19/20, ou seja, 95%.
Qual seria a precisão se fosse feito apenas uma árvore?

``` wp-block-code
balloons = readr::read_csv("balloons.csv")
# tratando todas as variaveis 
balloons = dplyr::mutate_if(balloons, is.character, as.factor)
balloons$Inflated = as.factor(balloons$Inflated)
# separando amostras teste/treino
set.seed(45)
inTrain = caret::createDataPartition(balloons$Inflated,p=0.5,list=F)
treino = balloons[inTrain,]
teste = balloons[-inTrain,]
# treinando o modelo
modFit = caret::train(Inflated~., method="rpart", data=balloons)
# aplicando o modelo no teste
predicao = predict(modFit,teste)
# avaliando o erro na amostra treino
confusionMatrix(teste$Inflated,predicao)
```

Note que nessa árvore, nosso modelo teve uma precisão de 80%. Bem menor
do que o modelo de florestas.

Agora, observe que construimos uma floresta com 20 árvores. O que
acontece com o erro do modelo conforme acrescentamos mais árvores?

Vamos avaliar o comportamento do erro conforme acrescentamos mais
árvores à floresta. Para isso, utilizaremos a base de dados `spam` para
melhor vizualização

``` wp-block-code
# chamando a base
library(kernlab)
data("spam")
# construindo floresta com 20 arvores
library(randomForest)
set.seed(23)
modelo = randomForest(type ~ ., data=spam, ntree=20)
```

``` wp-block-code
# observando o comportamento do erro em 20 árvores
erro_OOB <- data.frame(
  Arvores = rep(1:nrow(modelo$err.rate), times=2),
  Type = rep(c("spam", "nonspam"), each=nrow(modelo$err.rate)),
  Erro = c(modelo$err.rate[,"spam"], modelo$err.rate[,"nonspam"]))

ggplot(data=erro_OOB, aes(x=Arvores, y=Erro)) +
  geom_line(aes(color=Type),size=1.1)
```

``` wp-block-code
# construindo floresta com 50 arvores
set.seed(23)
modelo = randomForest(type ~ ., data=spam, ntree=50)
# observando o comportamento do erro em 50 árvores
erro_OOB <- data.frame(
  Arvores=rep(1:nrow(modelo$err.rate), times=2),
  Type=rep(c("spam", "nonspam"), each=nrow(modelo$err.rate)),
  Erro=c(modelo$err.rate[,"spam"],
          modelo$err.rate[,"nonspam"]))

ggplot(data=erro_OOB, aes(x=Arvores, y=Erro)) +
  geom_line(aes(color=Type),size=1.1)
```

``` wp-block-code
# construindo floresta com 100 arvores
set.seed(23)
modelo = randomForest(type ~ ., data=spam, ntree=100)
# observando o comportamento do erro em 100 árvores
erro_OOB <- data.frame(
  Arvores=rep(1:nrow(modelo$err.rate), times=2),
  Type=rep(c("spam", "nonspam"), each=nrow(modelo$err.rate)),
  Erro=c(modelo$err.rate[,"spam"],
          modelo$err.rate[,"nonspam"]))

ggplot(data=erro_OOB, aes(x=Arvores, y=Erro)) +
  geom_line(aes(color=Type),size=1.1)
```

``` wp-block-code
# construindo floresta com 1000 arvores
set.seed(23)
modelo = randomForest(type ~ ., data=spam, ntree=1000)
# observando o comportamento do erro em 1000 árvores
erro_OOB <- data.frame(
  Arvores=rep(1:nrow(modelo$err.rate), times=2),
  Type=rep(c("spam", "nonspam"), each=nrow(modelo$err.rate)),
  Erro=c(modelo$err.rate[,"spam"],
          modelo$err.rate[,"nonspam"]))

ggplot(data=erro_OOB, aes(x=Arvores, y=Erro)) +
  geom_line(aes(color=Type),size=1.1)
```

Repare que após uma certa quantidade de árvores, o erro se estabiliza.
Sendo assim, não é necessário utilizar grandes quantidades de árvores em
todos os casos. É preciso verificar até onde existe ganho.

### Construindo uma floresta com o `train()`

Também é possivel fazer florestas aleatórias usando a função `train` do
pacote `caret`. Para isso, é necessário alterar o método de reamostragem
para *out of bag*. Vamos utilizar a base `spam` para melhor
visualização.

``` wp-block-code
# alterando o metodo de reamostragem
controle = trainControl(method="oob")
# chamando a base
library(kernlab)
data(spam)
# construindo o modelo com 50 arvores
set.seed(534)
modelo = caret::train(type ∼ ., data=spam, method="rf", ntree=50, trControl=controle)
modelo
```

Note o valor "`mtry`" no modelo. Ele indica a quantidade de váriaveis da
base que foram utilizadas. Repare que ele calcula a precisão e kappa
para diferentes quantidades de variáveis usadas e utiliza no final a
quantidade que possuir maior precisão, no caso mtry=29. Caso queira
fixar o número de variáveis usadas, basta usar o seguinte comando.

``` wp-block-code
tng = expand.grid(.mtry=7)
modelo = caret::train(type ∼ ., data=spam, method="rf", ntree=50, trControl=controle, tuneGrid=tng)
modelo
```

------------------------------------------------------------------------

## AdaBoost

O método de treino AdaBoost se baseia na construção de uma floresta
aleatória. Entretanto, na floresta construída por esse método as árvores
possuem apenas um nó e duas folhas. Essas árvores são chamadas de
**tocos**.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-22-1024x458.png){.wp-image-204
loading="lazy" width="1024" height="458"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-22-980x439.png 980w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-22-480x215.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) and (max-width: 980px) 980px, (min-width: 981px) 1024px, 100vw"}

Em geral, tocos não são muito bons em fazer classificações precisas, ou
seja, eles são classificadores fracos. No entanto, o método AdaBoost os
combina de forma a criar um bom aprendiz. Ele faz isso utilizando
diferenciais na classificação e na construção das árvores que a floresta
aleatória comum não utiliza:

-   Floresta Aleatória: cada árvore de decisão tem um peso igual na
    classificação final das amostras. Além disso, cada árvore é
    construída independentemente das outras.
-   AdaBoost: alguns tocos têm mais peso na classificação final do que
    outros, e a ordem de construção dos tocos importam. Em outras
    palavras, os erros que o primeito toco comete influenciam em como o
    segundo toco é construído, os erros que o segundo toco comete
    influenciam em como o terceiro toco é construído, e assim
    sucessivamente.

Vamos ver os detalhes práticos de como funciona o AdaBoost utilizando o
banco de dados golf. Este banco possui informações sobre condições
climáticas e se o indivíduo jogou golf no dia. A ideia é tentar prever
se o indivíduo vai jogar golf baseado nas outras variáveis.

``` wp-block-code
golf = readRDS("Golf.rds")
golf
```

Primeiramente construímos um toco para cada uma das variáveis e
calculamos seus respectivos índices Gini. Vamos começar com a variável
*Outlook*.

``` wp-block-code
library(dplyr)
golf %>% group_by(Outlook, Play) %>% summarise(N=n())
```

Então temos que "Outlook = Overcast" separa os dados da seguinte forma:

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-9.png){.wp-image-188
loading="lazy" width="400" height="340"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-9.png 400w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-9-300x255.png 300w"
sizes="(max-width: 400px) 100vw, 400px"}

\\(\\hbox{Gini(Outlook = Overcast)} = \\frac{4}{14} \\times \\left\[ 1-
\\left( \\frac{4}{4} \\right)\^{2} -- \\left( \\frac{0}{4} \\right)\^{2}
\\right\] + \\frac{10}{14} \\times \\left\[ 1- \\left( \\frac{5}{10}
\\right)\^{2} -- \\left( \\frac{5}{10} \\right)\^{2} \\right\] =
0,357.\\)

Vamos agora olhar para "Outlook = Rain":

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-12.png){.wp-image-194
loading="lazy" width="400" height="340"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-12.png 400w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-12-300x255.png 300w"
sizes="(max-width: 400px) 100vw, 400px"}

\\(\\hbox{Gini(Outlook = Rain)} = \\frac{5}{14} \\times \\left\[ 1-
\\left( \\frac{3}{5} \\right)\^{2} -- \\left( \\frac{2}{5} \\right)\^{2}
\\right\] + \\frac{9}{14} \\times \\left\[ 1- \\left( \\frac{6}{9}
\\right)\^{2} -- \\left( \\frac{3}{9} \\right)\^{2} \\right\] =
0,457.\\)

Por último, "Outlook = Sunny":

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-10.png){.wp-image-191
loading="lazy" width="377" height="282"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-10.png 377w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-10-300x224.png 300w"
sizes="(max-width: 377px) 100vw, 377px"}

\\(\\hbox{Gini(Outlook = Sunny)} = \\frac{5}{14} \\times \\left\[ 1-
\\left( \\frac{2}{5} \\right)\^{2} -- \\left( \\frac{3}{5} \\right)\^{2}
\\right\] + \\frac{9}{14} \\times \\left\[ 1- \\left( \\frac{7}{9}
\\right)\^{2} -- \\left( \\frac{2}{9} \\right)\^{2} \\right\] =
0,394.\\)

Agora vamos para a variável *Humidity*.

``` wp-block-code
golf %>% group_by(Humidity, Play) %>% summarise(N=n())
```

Temos que "Humidity = High" separa os dados da seguinte forma:

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-11.png){.wp-image-193
loading="lazy" width="366" height="286"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-11.png 366w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-11-300x234.png 300w"
sizes="(max-width: 366px) 100vw, 366px"}

\\(\\hbox{Gini(Humidity = High)} = \\frac{7}{14} \\times \\left\[ 1-
\\left( \\frac{3}{7} \\right)\^{2} -- \\left( \\frac{4}{7} \\right)\^{2}
\\right\] + \\frac{7}{14} \\times \\left\[ 1- \\left( \\frac{6}{7}
\\right)\^{2} -- \\left( \\frac{1}{7} \\right)\^{2} \\right\] =
0,367.\\)

Por último, a variável *Wind*:

``` wp-block-code
golf %>% group_by(Wind, Play) %>% summarise(N=n())
```

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-13.png){.wp-image-195
loading="lazy" width="355" height="287"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-13.png 355w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-13-300x243.png 300w"
sizes="(max-width: 355px) 100vw, 355px"}

\\(\\hbox{Gini(Wind = Strong)} = \\frac{6}{14} \\times \\left\[ 1-
\\left( \\frac{3}{6} \\right)\^{2} -- \\left( \\frac{3}{6} \\right)\^{2}
\\right\] + \\frac{8}{14} \\times \\left\[ 1- \\left( \\frac{6}{8}
\\right)\^{2} -- \\left( \\frac{2}{8} \\right)\^{2} \\right\] =
0,429.\\)

Logo, os índices Gini calculados foram:

``` wp-block-code
gini = tibble("Variáveis" = c("Outlook = Overcast", "Outlook = Rain", "Outlook = Sunny", "Humidity = High", "Wind = Strong"), "Índice Gini" = c("0,357", "0,457", "0,394", "0,367", "0,429"))

knitr::kable(gini)
```

Selecionamos a variável com o menor índice Gini para ser o primeiro toco
da floresta. Nesse caso, o menor índice Gini é o da variável "Outlook =
Overcast".

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-14.png){.wp-image-196
loading="lazy" width="488" height="352"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-14.png 488w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-14-480x346.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 488px, 100vw"}

Agora precisamos calcular o peso desse toco na classificação final. Para
isso, vamos calcular seu **erro total**.

> O **erro total** de um toco é calculado pelo número de amostras
> classificadas erradas dividido pelo total de amostras.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-16.png){.wp-image-198
loading="lazy" width="474" height="430"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-16.png 474w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-16-300x272.png 300w"
sizes="(max-width: 474px) 100vw, 474px"}

Para esse toco houve 5 amostras classificadas erradas em um total de 14.
Logo,

\\\[ \\hbox{Erro Total} = \\frac{5}{14}. \\\]

Dessa forma podemos calcular o *Amount of Say* do toco, que será seu
peso na classificação final.

\\\[ \\hbox{Amount of Say} = \\frac{1}{2} \\times log \\left(
\\frac{1-\\hbox{Erro Total}}{\\hbox{Erro Total}} \\right) \\\] Logo, o
*Amount of Say* desse toco será de:

\\\[ \\hbox{Amount of Say} = \\frac{1}{2} \\times log \\left(
\\frac{1-5/14}{5/14} \\right) = 0,29.\\\]

Então 0,29 é o seu peso na classificação final.

Agora vamos construir o próximo toco. Para isso damos um peso maior para
as amostras que foram classificadas erroneamente no toco anterior. Essas
amostras foram as seguintes:

``` wp-block-code
golf %>% filter(Outlook != "Overcast" & Play != "No")
```

Então, para rebalancearmos os pesos das amostras classificadas de forma
certa e errada, utilizamos as seguintes fórmulas:

\\\[ \\hbox{Peso Amostras Erradas} = \\hbox{Erro Total} \\ \\times
e\^{\\hbox{Amount of Say}}\\\] \\\[ \\hbox{Peso Amostras Corretas} =
\\hbox{Erro Total} \\ \\times e\^{-\\hbox{Amount of Say}}\\\]

Assim, para o segundo toco, os pesos serão:

\\\[ \\hbox{Peso Amostras Erradas} = \\frac{5}{14} \\ \\times e\^{0,29}
= 0,477.\\\] \\\[ \\hbox{Peso Amostras Corretas} = \\frac{5}{14} \\
\\times e\^{-0,29} = 0,267.\\\]

Então temos os pesos para as amostras:

A soma dos pesos das amostras deve ser 1, mas isso não ocorre: note que
a soma resulta em 4,788. Dessa forma, precisamos reescalar os pesos.
Faremos isso dividindo cada um deles por 4,788.

Feito isso, temos uma nova tabela de pesos:

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-19.png){.wp-image-201
loading="lazy" width="345" height="301"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-19.png 345w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-19-300x262.png 300w"
sizes="(max-width: 345px) 100vw, 345px"}

Definidos os pesos, em seguida realizamos uma reamostragem via
*bootstrap* (uma amostragem da própria amostra, com reposição) do mesmo
tamanho da base de dados original. A probabilidade de um elemento da
amostra ser sorteado é o peso dele.

``` wp-block-code
# Numerando os elementos da amostra:
amostra = 1:14

# Definindo as probabilidades dos elementos serem sorteados:
pesos = rep(c(0.056, 0.099, 0.056, 0.099, 0.056), times = c(3,2,3,3,3))

# Realizando o bootstrap:
set.seed(271)
sample(amostra, size = 14, replace = T, prob = pesos)
```

Então temos uma nova amostra formada pelos elementos sorteados na
reamostragem:

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-40.png){.wp-image-224
loading="lazy" width="693" height="808"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-40.png 693w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-40-480x560.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 693px, 100vw"}

Agora, com essa nova amostra, fixamos pesos uniformes para os elementos
e repetimos o processo de criação para o próximo toco. Em seguida
verificamos os elementos que foram classificados de forma errada,
aumentamos seus pesos no banco de dados e repetimos o processo de
*bootstrap*, construindo, assim, o próximo toco. O processo se repete
até que a floresta de tocos esteja concluída.

Finalizada a floresta, realizamos a classificação final dos elementos
somando os pesos dos tocos para cada classificação e selecionando o
maior deles. Por exemplo, em uma floresta com 10 árvores onde 5 delas
classificam a amostra na categoria de interesse como "positivo" e 5
delas classificam essa mesma amostra como "negativo", se a soma dos
pesos das que classificaram a amostra como "positivo" for 2,7 e a das
que classificaram a amostra como "negativo" for 0,84, a amostra será
classificada como "positivo".

### Adaboost com o `train`

Agora que já sabemos como funciona o *adaboost*, vamos botá-lo em
prática através da função train(). Para isso basta escolhermos essa
opção no argumento referente ao método de treino que será utilizado.
Vamos fazer isso utilizando a base de dados College, utilizando o mesmo
conjunto treino e teste utilizado no [exemplo de árvores de
decisão](#link8), para compararmos os resultados.

``` wp-block-code
college = readr::read_csv2("College.csv")

# Separando a amostra em treino e teste:
set.seed(100)
noTreino = caret::createDataPartition(y = college$Private, p = 0.7, list = F)
treino = college[noTreino,]
teste = college[-noTreino,]

# Treinando o modelo com o adaboost:
modelo = caret::train(Private~., method = "adaboost", data = treino)
modelo
```

Vamos aplicar o modelo no conjunto teste e avaliá-lo através da matriz
de confusão.

``` wp-block-code
predicao = predict(modelo, teste)

# Transformando em fator para depois construirmos a matriz de confusão:
teste$Private = as.factor(teste$Private)

# Construindo a matriz de confusão:
confusionMatrix(predicao, teste$Private)
```

Note que obtivemos resultados melhores utilizando o adaboost do que
simplesmente uma árvore de decisão. A acurácia, em particular, subiu de
0,89 para 0,92.

### Adaboost com o pacote `adabag`

Também podemos utilizar o pacote `adabag` para treinarmos um modelo pelo
adaboost. Vamos utilizar a base de dados *spam*.

Inicialmente vamos separar a amostra em treino e teste.

``` wp-block-code
library(kernlab)
data(spam)
set.seed(16)
noTreino = createDataPartition(y = spam$type, p = 0.7, list = F)
treino = spam[noTreino,]
teste = spam[-noTreino,]
```

Antes de realizarmos o *adaboost* precisamos definir a profundidade
máxima que as árvores da floresta terão. Faremos isso através do comando
rpart.control(). Como o objetivo é construir uma floresta de tocos, as
árvores terão todas profundidade 1.

``` wp-block-code
library(rpart)
controle = rpart.control(maxdepth = 1)
```

Agora vamos aplicar o método *adaboost* no conjunto treino utilizando o
comando boosting().

``` wp-block-code
library(adabag)
modelo = boosting(formula = type~., data = treino, boos = T, mfinal = 100,
                  coeflearn = "Breiman", control = controle)
```

Os principais argumentos dessa função são:

-   **formula** = uma fórmula especificando qual variável queremos
    prever em função de qual(is);
-   **data** = base de dados onde se encontram as variáveis;
-   **boos** = argumento do tipo *logical* onde, se TRUE (*default*),
    utiliza *bootstrap* para criar uma nova amostra treino para a
    próxima árvore baseado nos erros da árvore anterior;
-   **mfinal** = número de árvores da floresta;
-   **coeflearn** = define qual fórmula será utilizada para o *Amount of
    Say* de cada árvore. A que vimos é a fórmula de Breiman (*default*);
-   **control** = opções que controlam detalhes do algoritmo rpart.

Para visualizarmos qualquer árvore da floresta utilizamos o comando
rpart.plot().

``` wp-block-code
library(rpart.plot)

# Visualizando a primeira árvore construída:
rpart.plot(modelo$trees[[1]])
```

Por último, vamos aplicar o modelo na amostra teste e em seguida avaliar
o modelo através da matriz de confusão.

``` wp-block-code
predicao = predict(modelo, teste)

# Resumo da matriz de confusão:
predicao$confusion
```

------------------------------------------------------------------------

## Gradiente Boosting

### em Regressão

De acordo com Jerome Friedman, o criador do Gradiente Boosting,
evidências empíricas mostram que dar pequenos passos ou ir
gradativamente na direção correta resulta em melhores predições na
amostra teste, ou seja, menor variância.

Para entendermos como funciona o Gradiente Boosting, considere a
seguinte base de dados.

``` wp-block-code
base = readr::read_csv("peso.csv")
base
```

A primeira coisa a fazer é definir um número máximo de folhas de cada
árvore. Para nosso exemplo, vamos definir 4 folhas, mas em geral, é
definido uma quantidade de 8 a 32 folhas. Feito isso, tiramos uma média
dos pesos dos indivíduos e essa será nossa primeira árvore, uma árvore
só com a raiz.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-17.png){.wp-image-199
loading="lazy" width="306" height="150"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-17.png 306w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-17-300x147.png 300w"
sizes="(max-width: 306px) 100vw, 306px"}

{width=30%}

Agora, calculamos o **Pseudo-Resíduo**, o erro de previsão de cada
indivíduo, da forma \\\[Pseudo\\ Resíduo = valor\\ real -- valor\\
predito\\\] para cada observação. Então, por exemplo, o pseudo-resíduo
da primeira observação vai ficar \\(88-71.5 = 16.5\\) .

``` wp-block-code
base1 = readr::read_csv("peso1.csv")
base1
```

> O termo **pseudo-resíduo** é baseado em Regressão Linear, onde o
> resíduo é a diferença entre os valores observados e estimados. O termo
> "pseudo" serve para lembrar que estamos fazendo **Gradiente Boosting**
> e não Regressão Linear.

O próximo passo é, utilizando as variáveis explicativas (Altura, Cor e
Sexo), construir uma árvore de decisão respeitando o máximo de folhas
definido anteriormente. Mas ela deve **predizer o pseudo-resíduo** e não
o Peso.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-18.png){.wp-image-200
loading="lazy" width="960" height="520"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-18.png 960w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-18-480x260.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 960px, 100vw"}

{width=85%}

> Note que temos mais observações do que folhas, sendo assim, podemos
> ter mais que um resultado em cada uma. Nesse caso, substituímos os
> valores pela média das folhas.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-24.png){.wp-image-206
loading="lazy" width="960" height="481"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-24.png 960w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-24-480x241.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 960px, 100vw"}

{width=85%}

Agora somamos o resultado das duas árvores para classificar na primeira
observação, por exemplo, a predição seria \\(71.5+16.5=88\\). Acertamos
exatamente o valor real. Isso é bom? Não. Já vimos como não é bom ter um
modelo muito ajustado. Temos pouco viés, mas provavelmente alta
variância.

O Gradiente Boosting lida com esse problema usando uma **taxa de
aprendizado** para reescalar a contribuição da nova árvore. A taxa de
aprendizado é um número entre 0 e 1 e deve ser multiplicado ao valor da
segunda árvore em diante. Para esse exemplo, vamos adotar uma taxa de
0.1, assim a predição da primeira observação seria
\\(71.5+(0.1\*16.5)=73.15\\) . A predição não ficou tão boa, mas é um
pouco melhor do que o resultado de apenas uma árvore.

Feito isso, recalculamos os valores do pseudo-resíduo.

``` wp-block-code
base2 = readr::read_csv("peso2.csv")
base2
```

Repare que o valor do segundo pseudo-resíduo diminuiu em módulo em
relação ao primeiro, ou seja, nos aproximamos mais do valor correto do
que da primeira vez.

Agora, utilizando novamente as variáveis explicativas, construímos outra
árvore agora para predizer o segundo pseudo-resíduo.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-20-1024x277.png){.wp-image-202
loading="lazy" width="1024" height="277"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-20-980x265.png 980w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-20-480x130.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) and (max-width: 980px) 980px, (min-width: 981px) 1024px, 100vw"}

{width=100%}

> Note que a estrutura da segunda árvore construída ficou semelhante a
> primeira. Isso não acontece sempre, mas pode acontecer.

Agora, a classificação da primeira observação ficaria
\\(71.5+(0.1\*16.5)+(0.1\*14.85)=74.635\\) um pouco mais perto do
verdadeiro valor. Repetimos esse procedimento quantas vezes se queira ou
até não ter redução significante dos valores do pseudo-resíduo. Dessa
forma temos uma sequência de árvores que caminham em direção ao valor
correto em passos pequenos.

> É importante notar que todas as árvores devem possuir a mesma taxa de
> aprendizado.

#### Construindo um regressor com o pacote `gbm`

Para nosso exemplo, vamos utilizar a base de dados `Wage` do pacote
`ISLR`. Para isso, vamos precisar limpar os dados removendo [variáveis
de variância zero](#link7).

``` wp-block-code
# lendo a base de dados
library(ISLR)
data("Wage")
# removendo variaveis de variancia zero
vvz = nearZeroVar(Wage,saveMetrics = F)
vvz
Wage = Wage[,-vvz]
```

Dividimos a base nos conjuntos de treino e teste.

``` wp-block-code
set.seed(100)
noTreino = createDataPartition(Wage$wage,p=0.7,list=F)
treino = Wage[noTreino,]
teste = Wage[-noTreino,]
```

Agora vamos aplicar o gradiente boosting com a função `gbm()`

``` wp-block-code
library(gbm)
modelo = gbm(wage~.,data=treino, distribution="gaussian",
              n.trees =300,interaction.depth = 20)
modelo
```

Os principais argumentos da função `gbm()` são:

-   **distribution**: *gaussian* se for regressão, *multinomial* se for
    um classificação, *bernoulli* se for classificação 0-1.
-   **n.trees**: número de árvores da floresta.
-   **interaction.depth**: profundidade máxima das árvores.

Vamos aplicar o modelo na amostra teste, e avaliar o resultado.

``` wp-block-code
predicao = predict(modelo, teste, n.trees=300)
# avaliando
postResample(predicao,teste$wage)
```

Note que utilizamos 300 árvores. Mas pode ser que não seja necessário
essa quantidade de árvores pra alcançar esses valores de \\(R\^2\\),
RMSE e MAE. Para saber a quantidade ideal de árvores, isto é, quando
erro se estabiliza, podemos utilizar a função `gbm.perf()`.

``` wp-block-code
gbm.perf(modelo)
```

Sendo assim, com apenas 50 árvores teríamos chegado a um resultado
razoável.

``` wp-block-code
predicao2 = predict(modelo, teste, n.trees=50)
postResample(predicao2,teste$wage)
```

### em Classificação

Considere a seguinte base de dados

``` wp-block-code
base = readr::read_csv("Troll 2.csv")
base
```

Queremos predizer se uma pessoa ama o filme Troll 2 baseado em seu gosto
por pipoca, idade e cor favorita. Assim como em regressão, começamos o
método de Gradiente Boosting usando uma árvore raiz que represente nossa
predição inicial para cada observação. Em regressão usamos a média das
observações, em classificação vamos usar o log(chances). Olhando na base
de dados, podemos dizer que as chances de alguém amar Troll 2 é
\\\[chances= \\frac{Quantidade\\ de\\ indivíduos\\ que\\
amaram}{Quantidade\\ de\\ indivíduos\\ que\\ odiaram} = \\frac{4}{2}\\\]
portanto, o \\(log(chances)=log(\\frac{4}{2}) = 0.6932\\) e é isso que
colocaremos na folha inicial.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-27.png){.wp-image-209
loading="lazy" width="345" height="201"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-27.png 345w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-27-300x175.png 300w"
sizes="(max-width: 345px) 100vw, 345px"}

{width=30%}

Assim como em regressão logística, o jeito mais fácil de usar o
log(chances) para classificar é convertendo em probabilidade, e fazemos
isso usando a seguinte função logística \\\[ Probabilidade =
\\frac{e\^{log(chances)}}{1+e\^{log(chances)}}\\\] Sendo assim, A
\\(Probabilidade\\ de\\ alguém\\ amar\\ Troll2 =
\\frac{e\^{log(\\frac{4}{2})}}{1+e\^{log(\\frac{4}{2})}} =
\\frac{2}{3}=0.6667\\).

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-23.png){.wp-image-205
loading="lazy" width="339" height="234"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-23.png 339w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-23-300x207.png 300w"
sizes="(max-width: 339px) 100vw, 339px"}

{width=30%}

> É importante notar que o log(chances) e a probabilidade só ficaram
> iguais por causa da aproximação.

Podemos usar um limite de corte 0.5, sendo assim, já que a probabilidade
ficou maior que o corte, nós classificamos todos no treino como
indivíduos que amam Troll 2.

> Embora 0.5 seja um limite usual para tomada de decisão baseada em
> probabilidade, poderiamos tranquilamente usar um valor diferente.

Mas a classificação não ficou muito boa já que 2 indivíduos foram
classificados erroneamente. Podemos mensurar quão ruim foi a predição
calculando o \\(pseudo\\ resíduo = observado -- predito\\). Para essa
conta, perceba que se um indivíduo ama Troll 2, então a probabilidade
dele amar Troll 2 é 1. Semelhantemente, se ele odeia, a probabilidade
dele amar é 0. Assim, calculamos os pseudo-resíduos.

``` wp-block-code
base = readr::read_csv("Troll2 1.csv")
base
```

Agora, construímos uma árvore utilizando as variáveis explicativas para
predizer o pseudo-resíduo. Assim como o Gradiente Boosting para
regressão, temos que definir um número máximo de folhas em cada árvore.
Aqui vamos limitar a 3 folhas, mas na prática geralmente é um número
entre 8 e 32.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-21.png){.wp-image-203
loading="lazy" width="728" height="522"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-21.png 728w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-21-480x344.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 728px, 100vw"}

{width=80%}

Em regressão, os valores das folhas representavam os resíduos. Mas em
classificação isso é mais complexo. Isso porque a predição está em
log(chances) e as folhas são provenientes de probabilidade. Portanto não
podemos apenas soma-las para uma nova predição sem alguma transformação.
A transformação mais comum por folha é \\\[\\frac{\\sum
resíduos}{\\sum\[prob.\\ anterior \* (1-prob.\\ anterior)\]}\\\]

Assim, da esquerda pra direita, para primeira folha temos
\\\[\\frac{\\sum resíduos}{\\sum\[prob.\\ anterior \* (1-prob.\\
anterior)\]}=\\frac{-0.7}{0.7\*(1-0.7)}=-3.3333\\\], para a segunda
\\\[\\frac{\\sum resíduos}{\\sum\[prob.\\ anterior \* (1-prob.\\
anterior)\]}=\\frac{0.3+(-0.7)}{(0.7\*(1-0.7))+(0.7\*(1-0.7))}=-0.9524\\\]
e para última \\\[\\frac{\\sum resíduos}{\\sum\[prob.\\ anterior \*
(1-prob.\\ anterior)\]}=\\\]
\\\[\\frac{0.3+0.3+0.3}{(0.7\*(1-0.7))+(0.7\*(1-0.7))+(0.7\*(1-0.7))}=\\frac{3\*0.3}{3\*(0.7\*(1-0.7))}=1.4286\\\]

> Por enquanto, a probabilidade anterior é a mesma para todos, mas a
> partir da próxima árvore isso muda.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-26.png){.wp-image-208
loading="lazy" width="708" height="566"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-26.png 708w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-26-480x384.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 708px, 100vw"}

{width=80%}

Agora que todas as folhas foram alteradas, podemos somar os resultados
escalados pela taxa de aprendizado. Nesse exemplo, vamos usar uma taxa
alta, 0.8. Mas geralmente se usa 0.1. E então calculamos o novo
\\(log(chances)=log(chances)\\ anterior + taxa\\ de\\ aprendizado \*
log(chances)\\ obtido\\ na\\ árvore\\). Para primeira observação, por
exemplo, fica \\(log(chances)=0.7+(0.8\*1.4)=1.82\\) e então convertemos
em probabilidade \\(\\frac{e\^{1.82}}{1+e\^{1.82}} = 0.8606\\). Então,
note que fizemos progresso, já que o indivíduo em questão ama Troll 2.
Antes ele foi classificado corretamente mas com probabilidade 0.7, agora
ele foi classificado corretamente mas com probabilidade 0.9.

``` wp-block-code
base = readr::read_csv("Troll2 2.csv")
base
```

> Pode ser que a previsão fique pior, como no caso do segundo indivíduo.
> E essa é a razão de construírmos várias árvores e não só uma.

Calculamos os novos pseudo-resíduos que agora serão diferentes para cada
observação.

``` wp-block-code
base = readr::read_csv("Troll2 3.csv")
base[,-5]
```

Construímos uma segunda árvore agora para prever os novos
pseudo-resíduos e fazemos a transformação para log(chances) para cada
folha.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-25.png){.wp-image-207
loading="lazy" width="704" height="568"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-25.png 704w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-25-480x387.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 704px, 100vw"}

{width=80%}

Combinamos com as árvores anteriores para obter um valor de saída e
transformamos em Probabilidade para classificar. Por exemplo, a primeira
observação ficaria \\(log(chances)=0.7+(0.8\*1.4)+(0.8\*0.6)=2.3\\) e
então convertemos em probabilidade \\(\\frac{e\^{1.82}}{1+e\^{1.82}} =
0.9089\\). Dessa forma, continuamos construíndo quantas árvores forem
necessárias.

#### Construindo um classificador com o pacote `gbm`

O gradiente boosting para classificação no R é semelhante ao para
regressão, atentando para o argumento `distribution`, que deve ser igual
a "bernoulli" se a variável de interesse tiver apenas duas respostas
possíveis (como no caso da bse Troll 2) ou "multinomial" se a variável
tiver mais de duas respostas possíveis. Por exemplo, considere a base
`Vehicle` do pacote `mlbench`. Nela, estamos interessados em classificar
a variável `Class`, que pode ser *bus, opel, saab* ou *van*.

``` wp-block-code
# lendo a base
library(mlbench)
data(Vehicle)
# dividindo em treino e teste
library(caret)
set.seed(100)
noTreino = createDataPartition(Vehicle$Class,p=0.7,list=F)
treino = Vehicle[noTreino,]
teste = Vehicle[-noTreino,]
# treinando o modelo
library(gbm)
modelo = gbm(Class~.,data=treino,distribution="multinomial",
              n.trees = 100,interaction.depth = 8)
```

Quando aplicamos o `predict()`, o que recebemos de retorno são um
conjunto de probabilidades (ou o log(chances)), e não a classificação
final. Cabendo ao pesquisador definir a regra de classificação final.

``` wp-block-code
predicao = predict(modelo, teste, n.trees=100, type = 'response')
```

O argumento ´type´ retorna por default o log(chances), se definimos como
"response" ele retorna a probabilidade.

``` wp-block-code
# Criando a regra de classificacao
k = dim(teste)[1]
classe = c()
for (i in 1:k){
  classe[i] = names(which.max(predicao[i,1:4,1])) 
}
head(classe)
# verificando quantidade de arvores necessarias
gbm.perf(modelo)
# avaliando o modelo 
confusionMatrix(data=as.factor(classe), reference=as.factor(teste$Class))
```

------------------------------------------------------------------------

## XGBoost {#link10}

O XGBoost é a abreviação de ***Extreme Gradient Boost***. Ele foi
desenvolvido para suportar um grande volume de dados de forma eficiente.
Geralmente é 10 vezes mais rápido que o Gradiente Boosting.

### Em Regressão

Apesar do XGBoost ser usado para lidar com bases grandes, vamos usar uma
base de dados bem pequena só para entendermos melhor como ele funciona.
Queremos predizer o peso de um indivíduo em função de sua altura.

``` wp-block-code
# lendo e vizualizando a base
library(readr)
(peso = read_csv("peso-altura.csv"))
library(ggplot2)
ggplot(peso, aes(x=Altura, y=Peso)) + geom_point(lwd=5, colour = "deeppink3") + 
  theme_minimal() + ylim(c(50,90)) + xlim(c(1.3,1.9))
```

\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\* O primeiro passo é fazer uma
predição inicial que pode ser qualquer uma. O defaut é usar 0.5, mas
como estamos falando de peso, vamos utilizar a predição inicial "Peso =
70".

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-28.png){.wp-image-210
loading="lazy" width="317" height="130"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-28.png 317w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-28-300x123.png 300w"
sizes="(max-width: 317px) 100vw, 317px"}

{width=80%}

Agora precisamos calcular os resíduos (diferença entre o valor real e o
valor predito) que vão nos mostrar quão boa é essa predição.

``` wp-block-code
library(dplyr)
(peso = peso %>% mutate( residuos = Peso-70 ))
```

Assim como no Gradiente Boosting, o próximo passo é construir uma árvore
para predizer os resíduos. Mas o XGBoost utiliza uma árvore de regressão
diferente que vamos chamar de árvore XGB. Existem muitas formas de
construir uma árvore XGB. Vamos aprender a mais comum. A árvore XGB
inicia com uma folha que leva todos os resíduos.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-29.png){.wp-image-212
loading="lazy" width="295" height="119"}

{width=80%}

Em seguida, calculamos um índice de qualidade ou **Índice de
similaridade** \\\[Índice\\ Similaridade\\ = \\frac{(soma\\ dos\\
resíduos)\^2}{número\\ de\\ resíduos + \\lambda} \\\] Onde
\\(\\lambda\\) (lambda) é um parâmetro de regularização, o que significa
que tem o objetivo de reduzir a sensibilidade das observações
individuais, ou seja, reduzir o sobreajuste. Por enquanto, vamos
considerar \\(\\lambda = 0\\) porque esse é o valor default. Sendo
assim, o Índice de similaridade da raiz é
\\(\\frac{(18+6-14+5-10)\^2}{5+0}=\\frac{5\^2}{5}=5\\)

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-41.png){.wp-image-225
loading="lazy" width="409" height="130"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-41.png 409w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-41-300x95.png 300w"
sizes="(max-width: 409px) 100vw, 409px"}

{width=80%}

Agora vamos ver se conseguimos melhorar esse índice dividindo os
resíduos, ou seja, criando uma ramificação. Vamos começar dividindo a
variável `Altura` na média entre os dois menores valores, que são
\\(1.4\\) e \\(1.5\\), e calculando o Indice para as novas folhas.

> Observe que nas folhas não estarão as alturas e sim os resíduos
> correspondentes a altura especificada.

\<img
src=\"http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-31.png\"
alt=\"Figura 4: Dividindo por \'Altura

{width=80%}

Agora, precisamos calcular o ganho dessa ramificação para ver o quanto
ela foi efetiva. O ganho é calculado da seguinte forma: \\\[ganho =
IS\_{folha\\ da\\ esquerda} + IS\_{folha\\ da\\ direita} --
IS\_{raiz}\\\] Assim, o ganho da ramificação 'Altura\<1.45' é \\(100 +
56.25 -- 5 = 151.25\\). Vamos fazer esse calculo em todas as
ramificações possiveis, Isto é, se temos 5 observações com diferentes
alturas, vamos ter 4 ramificações possiveis: 'Altura\<1.45',
'Altura\<1.55', 'Altura\<1.65' e 'Altura\<1.75'.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-32-1024x527.png){.wp-image-216
loading="lazy" width="1024" height="527"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-32-980x504.png 980w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-32-480x247.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) and (max-width: 980px) 980px, (min-width: 981px) 1024px, 100vw"}

{width=100%}

Podemos ver que o ganho de usar a ramificação 'Altura\<1.55' é maior,
portanto é essa que vamos usar. Agora vamos ramificar as folhas da mesma
maneira e escolher as que tiverem melhor ganho.

> Nesse exemplo, vamos limitar a profundidade da árvore XGB em 2. Mas o
> default é permitir até 6 níveis de profundidade.

Nossa árvore XGB final ficou:

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-35-1024x585.png){.wp-image-219
loading="lazy" width="1024" height="585"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-35-1024x585.png 1024w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-35-980x560.png 980w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-35-480x274.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) and (max-width: 980px) 980px, (min-width: 981px) 1024px, 100vw"}

{width=80%}

Agora, vamos podar nossa árvore. Fazemos isso porque pode ser que algum
nó tenha o ganho muito baixo e por isso não vale a pena estar na árvore.
Para decidir se vamos tirar algum nó e, se sim, qual, vamos escolher um
valor que será chamado de \\(\\gamma\\) (gamma). Em seguida, calculamos
a diferença entre o ganho associado ao nó e \\(\\gamma\\), se essa
diferença for negativa, então removemos o nó.

\\(\\gamma\\) especifica o ganho mínimo necessario para fazer uma
divisão. Seu default é 0. Quanto maior, mais conservador é o modelo.

> Mesmo quando \\(\\gamma = 0\\) isso não previne podas.

Vamos escolher \\(\\gamma = 10\\). Começando sempre dos nós mais
profundo para a raiz, vamos avaliar a diferença entre o ganho e
\\(\\gamma\\). No nó mais a direita, temos que o ganho é 32.7, portanto
a diferença é \\(32.7-10=22.7\\) como o resultado é positivo, o nó
permanece. No nó a esquerda, a diferença fica \\(8-10=-2\\), como o
resultado é positivo, retiramos esse nó. Assim, estamos dizendo que o
ganho do nó à esquerda não é bom o suficiente pra justificar essa
ramificação. Como o nó à direita permaneceu na árvore, não faz sentido
calcular essa diferença para o nó raiz.

> Mesmo se o valor da diferença der negativo nos nós de cima, se não
> removermos o de baixo, o de cima não é removido.

Com isso, nossa árvore XGB ficou:

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-30.png){.wp-image-213
loading="lazy" width="876" height="603"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-30.png 876w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-30-480x330.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 876px, 100vw"}

{width=80%}

Note que se tivéssemos escolhido um \\(\\gamma\\) muito alto, por
exemplo \\(\\gamma = 570\\), toda árvore seria podada. É preciso
cuidado.

Agora vamos voltar ao inicio e reconstruir a árvore agora usando
\\(\\lambda = 1\\) (lembra do \\(\\lambda\\)? aquele da fórmula do
índicador de similaridade 🙂 ). Para facilitar a vizualização, vamos
omitir os cálculos. A nova árvore XGB ficou:

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-34-1024x394.png){.wp-image-218
loading="lazy" width="1024" height="394"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-34-980x378.png 980w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-34-480x185.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) and (max-width: 980px) 980px, (min-width: 981px) 1024px, 100vw"}

{width=100%}

Podemos notar que quando \\(\\lambda \> 0\\), o índice de similaridade é
menor. O que significa que se mantivermos o mesmo \\(\\gamma\\), a poda
será mais extrema. Por outro lado, deixar \\(\\lambda \> 0\\) ajuda a
previnir sobreajustes.

Agora que temos árvore final, vamos calcular os valores de saída das
folhas. \\\[valores\\ de\\ saida = \\frac{soma\\ dos\\
residuos}{número\\ de\\ resíduos + \\lambda}\\\] Repare que essa fórmula
é bem parecida com a do índice de similaridade, mas a soma dos resíduos
não está ao quadrado.

Repare que, como \\(\\lambda = 0\\) o valor de saida é uma média
aritmética simples entre os resíduos. Mas note que se \\(\\lambda \>
0\\) e a folha tiver apenas uma observação, isso reduzira a
sensibilidade dessa observação individual evitando sobreajuste.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-33.png){.wp-image-217
loading="lazy" width="719" height="671"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-33.png 719w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-33-480x448.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 719px, 100vw"}

{width=80%}

Assim a primeira árvore está completa e, como em Gradient Boosting,
fazemos novas predições começando com a predição inicial e somando com o
resultado da árvore XGB escalada pela taxa de aprendizado.

> O XGBoost chama a taxa de aprendizado de \\(\\varepsilon\\) (eta) e
> seu valor default é 0.3, que é o que vamos usar.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-36-1024x492.png){.wp-image-220
loading="lazy" width="1024" height="492"
srcset="http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-36-980x471.png 980w, http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-36-480x231.png 480w"
sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) and (max-width: 980px) 980px, (min-width: 981px) 1024px, 100vw"}

{width=80%}

Por exemplo, se a gente pegasse a primeira observação (indivíduo com
altura=1.7), seu peso predito seria \\(predicao\\ inicial +
\\varepsilon\*valor\\ de\\ saida\\ da \\ árvore\\ XGB =
70+0.3\*12=73.6\\) que é mais perto do seu peso real (que era 88) do que
a predição anterior (70). Assim, com as novas predições, os novos
resíduos ficaram:

``` wp-block-code
nova_pred = c(73.6, 73.6, 66.4, 71.5, 66.4)
(peso = peso %>% mutate( residuos2 = Peso - nova_pred ))
```

Perceba que o novo resíduo é melhor que o anterior (seu valor absoluto é
mais próximo de 0). Ou seja, estamos dando pequenos passos na direção
correta.

Agora construímos outra árvore XGB da mesma forma, mas para predizer os
novos resíduos, Dessa forma obteremos previsões com resíduos menores. E
continuamos construíndo árvores XGB até que os resíduos sejam bem
pequenos ou até atingir o número de árvores desejado.

### Em Classificação

Para entendermos como o XGBoost funciona para problemas de
classificação, vamos utilizar a base de dados a seguir. O objetivo é
prever se a universidade é pública ou privada baseado nos pedidos para
ingresso.

> OBS: Assim como comentado anteriormente em regressão, o XGBoost foi
> projetado para bases de dados grandes, mas para fins didáticos iremos
> utilizar uma base bem pequena.

``` wp-block-code
library(readxl)
college = read_excel("SmallCollege.xlsx")
college
```

``` wp-block-code
library(ggplot2)
library(dplyr)
college %>% ggplot(aes(x = Apps, y = Private)) + geom_point(lwd = 5, aes(colour = Private)) +
  guides(col = F) + theme_minimal()
```

O primeiro passo é fazer uma predição inicial. Essa predição pode ser
qualquer valor, como por exemplo a probabilidade de observar
universidades públicas no conjunto de dados. Por *default*, essa
predição é de 0,5.

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-39.png){.wp-image-223
loading="lazy" width="278" height="164"}

Podemos ilustrar essa predição inicial adicionando uma linha horizontal
no gráfico que representa as probabilidades de uma universidade ser
pública pelo que observamos no conjunto de dados.

``` wp-block-code
college$Probabilidade = ifelse(college$Private == "Yes", 0, 1)
college %>% ggplot(aes(x = Apps, y = Probabilidade)) + 
  geom_point(lwd = 5, aes(colour = Private)) + theme_minimal() + 
  ylab("Probabilidade da Universidade ser Pública") + geom_hline(yintercept = 0.5, type = 2)
```

Feita a predição inicial, agora vamos calcular os resíduos e verificar
quão boa é essa predição.

``` wp-block-code
college$Residuos = college$Probabilidade-0.5
college
```

O próximo passo é construir uma árvore para predizer os resíduos. Assim
como a árvore XGB para regressão, a árvore XGB para classificação se
inicia com apenas uma folha que leva todos os resíduos.

![](Res%C3%ADduos_XGB.png)

Agora precisamos calcular o Índice de Similaridade para os resíduos.
Porém, como estamos usando XGBoost para classificação, temos uma nova
fórmula para ele.

\\\[ \\hbox{Índice de Similaridade} = \\frac{\\left(
\\sum\\limits\_{i=1}\^{n} \\hbox{Resíduo}\_i
\\right)\^{2}}{\\sum\\limits\_{i=1}\^{n} \\left\[ \\hbox{Probabilidade
Prévia}\_i \\times \\left( 1 -- \\hbox{Probabilidade Prévia}\_i \\right)
\\right\] + \\lambda} \\\]

Veja que o numerador da fórmula para classificação é igual ao da fórmula
para regressão. E assim como para regressão, o denominador contém
\\(\\lambda\\), o parâmetro de regularização.

Note que o numerador do Índice de Similaridade para a folha resultará em
0, pois nós somamos os resíduos antes de elevá-los ao quadrado, o que
faz com que eles se cancelem.

\\\[\\left( \\sum\\limits\_{i=1}\^{n} \\hbox{Resíduo}\_i \\right)\^{2} =
(0,5 -- 0,5 -- 0,5 + 0,5)\^{2} = 0 \\Rightarrow \\hbox{Índice de
Similaridade} = 0\\\]

![](http://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-37.png){.wp-image-221
loading="lazy" width="367" height="234"
srcset="https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-37.png 367w, https://cienciadedados.uff.br/wp-content/uploads/sites/195/2020/04/image-37-300x191.png 300w"
sizes="(max-width: 367px) 100vw, 367px"}

Vamos tentar melhorar o Índice dividindo os resíduos em 2 grupos
diferentes.

#### Construindo um regressor com o pacote `xgboost`

### Em Classificação

Vamos usar a base de dados `winequality-red`. O objetivo dessa base é
prever a qualidade do vinho baseado em suas outras variáveis. Mas nós
vamos tentar prever o nível alcoólico do vinho.

``` wp-block-code
library(readr)
wine = read_csv2("winequality-red.csv")
str(wine)
```

Como sempre, vamos dividir a base em amostra de treino e amostra de
teste.

``` wp-block-code
library(caret)
set.seed(100)
noTreino = createDataPartition(wine$alcohol ,p=0.7,list=F)
# vendo a classe da base de dados
class(wine)
```

O pacote `xgboost` só le matrizes. Então teremos que transformar a base
numa matriz. Além disso, teremos que separar a váriavel de interesse das
variáveis explicativas.

``` wp-block-code
# Transformando a base em matriz
wine = as.matrix(wine)
class(wine)
# separando amostra treino e teste
treino       = wine[noTreino,-11] # a variável 'alcohol' é a 11ª coluna
treino_label = wine[noTreino, 11]

teste       = wine[-noTreino,-11]
teste_label = wine[-noTreino, 11]
```

Agora, podemos usar a função `xgboost` para criar nosso modelo.

``` wp-block-code
library(xgboost)
modelo = xgboost(data = treino, label = treino_label,
                 gamma=0, eta=0.3, 
                 nrounds = 100, objective = "reg:squarederror")
```

``` wp-block-code
predicao = predict(modelo,teste)
mse = sum((teste_label-predicao)^2)/length(predicao)
```

------------------------------------------------------------------------

# Referências

JAMES, G. *et al*. **An Introduction to Statistical Learning**: with
applications in R. New York: Springer, 2013.

HASTIE, T.; TIBSHIRANI, R.; FRIEDMAN, J. **The Elements of Statistical
Learning**: Data Mining, Inference, and Prediction. Stanford: Springer,
2008.

PRACTICAL Machine Learning. **Coursera**. Disponível em
<https://www.coursera.org/learn/practical-machine-learning>. Acesso em
2019.

STARMER, Josh. **Machine Learning**, 2018. Disponível em
<https://www.youtube.com/watch?v=Gv9_4yMHFhI&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF>.
Acesso em 2019.

R Core Team (2019). R: A language and environment for statistical
computing. R Foundation for Statistical Computing, Vienna, Austria. URL
<https://www.R-project.org/>.

------------------------------------------------------------------------
